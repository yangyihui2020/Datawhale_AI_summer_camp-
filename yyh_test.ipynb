{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#导入初始数据\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv('用户新增预测挑战赛公开数据/train.csv')\n",
    "test_data = pd.read_csv('用户新增预测挑战赛公开数据/test.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 独热编码\n",
    "def udmap_onethot(d):\n",
    "    v = np.zeros(9)\n",
    "    if d == 'unknown':\n",
    "        return v\n",
    "\n",
    "    d = eval(d)\n",
    "    for i in range(1, 10):\n",
    "        if 'key' + str(i) in d:\n",
    "            v[i-1] = d['key' + str(i)]\n",
    "\n",
    "    return v\n",
    "\n",
    "train_udmap_df = pd.DataFrame(np.vstack(train_data['udmap'].apply(udmap_onethot)))\n",
    "test_udmap_df = pd.DataFrame(np.vstack(test_data['udmap'].apply(udmap_onethot)))\n",
    "\n",
    "train_udmap_df.columns = ['key' + str(i) for i in range(1, 10)]\n",
    "test_udmap_df.columns = ['key' + str(i) for i in range(1, 10)]\n",
    "\n",
    "train_data = pd.concat([train_data, train_udmap_df], axis=1)\n",
    "test_data = pd.concat([test_data, test_udmap_df], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TargetEncoder.__init__() got an unexpected keyword argument 'handle_unknown'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TargetEncoder\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# 创建 TargetEncoder 对象\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m encoder \u001B[38;5;241m=\u001B[39m \u001B[43mTargetEncoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle_unknown\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mvalue\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcols\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mkey\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# 在训练集上拟合目标编码器\u001B[39;00m\n\u001B[0;32m      8\u001B[0m encoder\u001B[38;5;241m.\u001B[39mfit(train_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mudmap\u001B[39m\u001B[38;5;124m'\u001B[39m], train_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "\u001B[1;31mTypeError\u001B[0m: TargetEncoder.__init__() got an unexpected keyword argument 'handle_unknown'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from feature_engine.encoding import TargetEncoder\n",
    "\n",
    "# 创建 TargetEncoder 对象\n",
    "encoder = TargetEncoder(handle_unknown='value', cols=['key' + str(i) for i in range(1, 10)])\n",
    "\n",
    "# 在训练集上拟合目标编码器\n",
    "encoder.fit(train_data['udmap'], train_data['target'])\n",
    "\n",
    "# 对训练集和测试集进行目标编码转换\n",
    "train_data_encoded = train_data.copy()\n",
    "train_data_encoded[['key' + str(i) for i in range(1, 10)]] = encoder.transform(train_data['udmap'])\n",
    "\n",
    "test_data_encoded = test_data.copy()\n",
    "test_data_encoded[['key' + str(i) for i in range(1, 10)]] = encoder.transform(test_data['udmap'])\n",
    "\n",
    "train_data = pd.concat([train_data, train_data_encoded], axis=1)\n",
    "test_data = pd.concat([test_data, test_data_encoded], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    数据处理"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data['eid_freq'] = train_data['eid'].map(train_data['eid'].value_counts())\n",
    "test_data['eid_freq'] = test_data['eid'].map(train_data['eid'].value_counts())\n",
    "\n",
    "train_data['eid_mean'] = train_data['eid'].map(train_data.groupby('eid')['target'].mean())\n",
    "test_data['eid_mean'] = test_data['eid'].map(train_data.groupby('eid')['target'].mean())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data['udmap_isunknown'] = (train_data['udmap'] == 'unknown').astype(int)\n",
    "test_data['udmap_isunknown'] = (test_data['udmap'] == 'unknown').astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data['common_ts'] = pd.to_datetime(train_data['common_ts'], unit='ms')\n",
    "test_data['common_ts'] = pd.to_datetime(test_data['common_ts'], unit='ms')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data['common_ts_hour'] = train_data['common_ts'].dt.hour\n",
    "test_data['common_ts_hour'] = test_data['common_ts'].dt.hour"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 将毫秒时间戳转换为日期时间类型\n",
    "train_data_temp_common_ts = pd.to_datetime(train_data['common_ts'], unit='ms')\n",
    "test_data_temp_commonn_ts = pd.to_datetime(test_data['common_ts'], unit='ms')\n",
    "\n",
    "# 提取星期几的数字表示作为新的列\n",
    "train_data['common_ts_weekday'] = train_data_temp_common_ts.dt.dayofweek\n",
    "test_data['common_ts_weekday'] = test_data_temp_commonn_ts.dt.dayofweek\n",
    "\n",
    "# 提取月份信息作为新的列\n",
    "train_data['common_ts_month'] = train_data_temp_common_ts.dt.month\n",
    "test_data['common_ts_month'] = test_data_temp_commonn_ts.dt.month"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    异常值处理"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设训练集数据已经导入到名为 train_df 的 Pandas DataFrame 中，包含字段 x1 至 x7\n",
    "\n",
    "# 定义异常值判断阈值（根据箱线图的定义，通常将小于下边界或大于上边界的值视为异常值）\n",
    "lower_bound = train_data[['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7']].quantile(0.25) - 1.5 * (train_data[['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7']].quantile(0.75) - train_data[['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7']].quantile(0.25))\n",
    "upper_bound = train_data[['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7']].quantile(0.75) + 1.5 * (train_data[['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7']].quantile(0.75) - train_data[['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7']].quantile(0.25))\n",
    "\n",
    "# 判断并处理异常值\n",
    "for col in ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7']:\n",
    "    train_data = train_data[(train_data[col] >= lower_bound[col]) & (train_data[col] <= upper_bound[col])]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_data['x1_freq'] = train_data['x1'].map(train_data['x1'].value_counts())\n",
    "# test_data['x1_freq'] = test_data['x1'].map(train_data['x1'].value_counts())\n",
    "train_data['x1_mean'] = train_data['x1'].map(train_data.groupby('x1')['target'].mean())\n",
    "test_data['x1_mean'] = test_data['x1'].map(train_data.groupby('x1')['target'].mean())\n",
    "\n",
    "train_data['x2_freq'] = train_data['x2'].map(train_data['x2'].value_counts())\n",
    "test_data['x2_freq'] = test_data['x2'].map(train_data['x2'].value_counts())\n",
    "train_data['x2_mean'] = train_data['x2'].map(train_data.groupby('x2')['target'].mean())\n",
    "test_data['x2_mean'] = test_data['x2'].map(train_data.groupby('x2')['target'].mean())\n",
    "\n",
    "# train_data['x3_freq'] = train_data['x3'].map(train_data['x3'].value_counts())\n",
    "# test_data['x3_freq'] = test_data['x3'].map(train_data['x3'].value_counts())\n",
    "# train_data['x3_mean'] = train_data['x3'].map(train_data.groupby('x3')['target'].mean())\n",
    "# test_data['x3_mean'] = test_data['x3'].map(train_data.groupby('x3')['target'].mean())\n",
    "\n",
    "# train_data['x4_freq'] = train_data['x4'].map(train_data['x4'].value_counts())\n",
    "# test_data['x4_freq'] = test_data['x4'].map(train_data['x4'].value_counts())\n",
    "train_data['x4_mean'] = train_data['x4'].map(train_data.groupby('x4')['target'].mean())\n",
    "test_data['x4_mean'] = test_data['x4'].map(train_data.groupby('x4')['target'].mean())\n",
    "\n",
    "# train_data['x5_freq'] = train_data['x5'].map(train_data['x5'].value_counts())\n",
    "# test_data['x5_freq'] = test_data['x5'].map(train_data['x5'].value_counts())\n",
    "train_data['x5_mean'] = train_data['x5'].map(train_data.groupby('x5')['target'].mean())\n",
    "test_data['x5_mean'] = test_data['x5'].map(train_data.groupby('x5')['target'].mean())\n",
    "\n",
    "# train_data['x6_freq'] = train_data['x6'].map(train_data['x6'].value_counts())\n",
    "# test_data['x6_freq'] = test_data['x6'].map(train_data['x6'].value_counts())\n",
    "# train_data['x6_mean'] = train_data['x6'].map(train_data.groupby('x6')['target'].mean())\n",
    "# test_data['x6_mean'] = test_data['x6'].map(train_data.groupby('x6')['target'].mean())\n",
    "#\n",
    "# train_data['x7_freq'] = train_data['x7'].map(train_data['x7'].value_counts())\n",
    "# test_data['x7_freq'] = test_data['x7'].map(train_data['x7'].value_counts())\n",
    "train_data['x7_mean'] = train_data['x7'].map(train_data.groupby('x7')['target'].mean())\n",
    "test_data['x7_mean'] = test_data['x7'].map(train_data.groupby('x7')['target'].mean())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    数据筛选，绘制相关性热力图"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# 选择相关的特征列和目标列\n",
    "feature_columns = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8',\n",
    "                   # 'x1_mean','x1_freq','x2_mean','x2_freq','x3_mean','x3_freq','x4_mean','x4_freq',\n",
    "                   # 'x5_mean','x5_freq','x6_mean','x6_freq','x7_mean','x7_freq',]\n",
    "                   'uuid', 'eid', 'common_ts', 'udmap_isunknown', 'eid_freq', 'eid_mean', 'common_ts', 'common_ts_hour','common_ts_weekday']\n",
    "\n",
    "target_column = 'target'\n",
    "\n",
    "# 计算特征与目标之间的相关性\n",
    "correlation_matrix = train_data[feature_columns + [target_column]].corr()\n",
    "# 输出与目标相关性最大的五个特征\n",
    "top_correlated_features = correlation_matrix[target_column].abs().nlargest(6)[1:]\n",
    "print(\"与目标相关性最大的五个特征:\")\n",
    "print(top_correlated_features)\n",
    "# 输出与目标相关性最小的三个特征\n",
    "bottom_correlated_features = correlation_matrix[target_column].abs().nsmallest(3)\n",
    "print(\"与目标相关性最小的三个特征:\")\n",
    "print(bottom_correlated_features)\n",
    "\n",
    "# 绘制热力图\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    判断数据类别"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设数据集已经导入到名为df的Pandas DataFrame中\n",
    "\n",
    "# 选择字段x1至x8\n",
    "fields = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8']\n",
    "\n",
    "\n",
    "# 判断字段数据类型\n",
    "data_types = []\n",
    "for field in fields:\n",
    "    unique_values = train_data[field].nunique()\n",
    "    if unique_values <= 2:\n",
    "        data_types.append('categorical')\n",
    "    else:\n",
    "        data_types.append('numerical')\n",
    "\n",
    "# 输出字段数据类型\n",
    "for field, data_type in zip(fields, data_types):\n",
    "    print(f\"{field}: {data_type}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 计算频数或比例\n",
    "counts = train_data.groupby(['x8', 'target']).size().unstack()\n",
    "\n",
    "# 绘制堆叠条形图\n",
    "counts.plot(kind='bar', stacked=True)\n",
    "\n",
    "# 设置图表标题和坐标轴标签\n",
    "plt.title('Relationship between x8 and target')\n",
    "plt.xlabel('x8 values')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    数据分桶\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "#\n",
    "# # 假设训练集数据已经导入到名为 train_df 的 Pandas DataFrame 中，包含字段 x1 至 x7\n",
    "#\n",
    "# # 定义分桶的数量\n",
    "# num_bins = 10\n",
    "#\n",
    "# # 对特征 x1 至 x7 进行等宽分桶\n",
    "# for col in ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7']:\n",
    "#     # 计算每个特征的分位数\n",
    "#     quantiles = np.linspace(0, 100, num_bins + 1)\n",
    "#     bin_edges = np.percentile(train_data[col], quantiles)\n",
    "#\n",
    "#     # 使用 pd.cut 函数进行分桶\n",
    "#     # 使用 pd.cut 函数进行分桶，并删除重复的边界值\n",
    "#     train_data[col + '_binned'] = pd.cut(train_data[col], bins=bin_edges, labels=False, include_lowest=True, duplicates='drop')\n",
    "# # 对特征 x1 至 x7 进行等宽分桶\n",
    "# for col in ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7']:\n",
    "#     # 计算每个特征的分位数\n",
    "#     quantiles = np.linspace(0, 100, num_bins + 1)\n",
    "#     bin_edges = np.percentile(test_data[col], quantiles)\n",
    "#\n",
    "#     # 使用 pd.cut 函数进行分桶\n",
    "#     test_data[col + '_binned'] = pd.cut(test_data[col], bins=bin_edges, labels=False, include_lowest=True, duplicates='drop')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    模型训练样本"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(\n",
    "    train_data.drop(['udmap', 'common_ts', 'uuid', 'target'], axis=1),\n",
    "    train_data['target']\n",
    ")\n",
    "# 对训练数据进行预测并计算F1分数\n",
    "train_predictions = clf.predict(train_data.drop(['udmap', 'common_ts', 'uuid', 'target'], axis=1))\n",
    "train_f1_score = f1_score(train_data['target'], train_predictions)\n",
    "\n",
    "print(\"Train F1 Score:\", train_f1_score)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'uuid': test_data['uuid'],\n",
    "    'target': clf.predict(test_data.drop(['udmap', 'common_ts', 'uuid'], axis=1))\n",
    "}).to_csv('submit.csv', index=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    个人训练测试"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
